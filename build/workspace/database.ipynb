{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d547959-e63a-410f-b5d1-00287c3e9f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shapely in /usr/local/lib/python3.9/dist-packages (2.0.0)\n",
      "Requirement already satisfied: geopandas in /usr/local/lib/python3.9/dist-packages (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.9/dist-packages (from shapely) (1.23.5)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in /usr/local/lib/python3.9/dist-packages (from geopandas) (3.4.1)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from geopandas) (1.4.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from geopandas) (22.0)\n",
      "Requirement already satisfied: fiona>=1.8 in /usr/local/lib/python3.9/dist-packages (from geopandas) (1.8.22)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from fiona>=1.8->geopandas) (52.0.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (2022.12.7)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (22.1.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: munch in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (8.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->geopandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/18 14:18:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n"
     ]
    }
   ],
   "source": [
    "!pip install shapely geopandas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from shapely import wkb\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely import wkb, Point, LineString\n",
    "from pyspark.sql import Row\n",
    "import pyspark.pandas as ps\n",
    "import math\n",
    "\n",
    "ps.set_option('compute.default_index_type', 'distributed')\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"spark://spark-master:7077\").config(\"spark.executor.memory\", \"512m\").getOrCreate()\n",
    "\n",
    "#dataframe = spark.read.format(\"jdbc\").options(url=\"jdbc:postgresql://db:5432/datascience?user=postgres&password=mantequilla\", dbtable=\"rutas\",driver=\"org.postgresql.Driver\").load()\n",
    "buses = spark.read.option(\"header\",True).csv(\"./data/buses.csv\")\n",
    "paraderos2 = spark.read.option(\"header\",True).csv(\"./data/paraderos2.csv\")\n",
    "paraderosrutas = spark.read.option(\"header\",True).csv(\"./data/paraderosrutas.csv\")\n",
    "print(\"Hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce897658-5556-439c-ab0c-5030150a8e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey\n"
     ]
    }
   ],
   "source": [
    "print(\"Hey\")\n",
    "#jdbcDF = spark.read\n",
    "#  .format(\"jdbc\")\n",
    "#  .option(\"url\", \"jdbc:postgresql:dbserver\")\n",
    "#  .option(\"dbtable\", \"schema.tablename\")\n",
    "#  .option(\"user\", \"username\")\n",
    "#  .option(\"password\", \"password\")\n",
    "#  .load()\n",
    "\n",
    "def distance(p0, p1):\n",
    "    return math.sqrt((p0[0] - p1[0])**2 + (p0[1] - p1[1])**2)\n",
    "def length(s) -> int:\n",
    "    return len(s)\n",
    "def some(item):\n",
    "    punto_bus = wkb.loads(item[\"geom_bus\"], hex=True)\n",
    "    punto_paradero = wkb.loads(item[\"parder_geom\"], hex=True)\n",
    "    distancia = punto_bus.distance(punto_paradero)\n",
    "    #x,y = punto.xy\n",
    "    #px, py = list(x)[0], list(y)[0]\n",
    "    temp = item\n",
    "    temp[\"distancia\"] = distancia\n",
    "    return temp  #punto #(px, py)\n",
    "\n",
    "def createData(buses, paraderos2, paraderosrutas):\n",
    "    # Observacion: Hay que cambiar la obtencion de parder_geom, puesto que debiese venir de la tabla paraderos2 pero el csv de stops.csv venÃ­a sin ese campo\n",
    "    busesrutas = paraderosrutas.join(buses, buses.vendor == paraderosrutas.cod_sinser).select(paraderosrutas.simt, \n",
    "        buses.bus, buses.fechahora, buses.lat, buses.lng, buses.velocidad, buses.pasajeros, buses.vendor, buses.id, buses.geom.alias(\"geom_bus\"),\n",
    "        paraderosrutas.servicio, paraderosrutas.nombre_par, paraderosrutas.geom.alias(\"parder_geom\"))\n",
    "    \n",
    "    \"\"\"\n",
    "    data = busesrutas.join(paraderos2, paraderos2.stop_id == busesrutas.simt, 'inner').select(busesrutas.columns + [paraderos2.stop_id.alias('parader'), \n",
    "        paraderos2.stop_name, paraderos2.stop_lat.alias('parder_lat'), paraderos2.stop_lon.alias('parder_lon'), busesrutas.geom_bus\n",
    "    ])\n",
    "    \"\"\"\n",
    "    data = busesrutas.join(paraderos2, paraderos2.stop_id == busesrutas.simt, 'inner').select(busesrutas.simt, busesrutas.bus, busesrutas.fechahora, busesrutas.lat, busesrutas.lng, busesrutas.velocidad, busesrutas.pasajeros, busesrutas.vendor, busesrutas.id, busesrutas.geom_bus, busesrutas.servicio, busesrutas.nombre_par, busesrutas.parder_geom, paraderos2.stop_id.alias('parader'), paraderos2.stop_name, paraderos2.stop_lat.alias('parder_lat'), paraderos2.stop_lon.alias('parder_lon'))\n",
    "    #data.printSchema()\n",
    "    geom = data.select(data.columns).dropna().coalesce(3)\n",
    "\n",
    "    #data.foreachPartition(some)\n",
    "    data_pd = geom.to_pandas_on_spark()#.coalesce(3)\n",
    "    data_pd_chunk = data_pd.spark.coalesce(3)\n",
    "\n",
    "    new_geom = data_pd_chunk.apply(some, axis='columns')\n",
    "    # axis = 1 --> foreach row : len(row) : numbers of columns\n",
    "    # axis = 1 --> foreach column : len(column) : numbers of rows by columns\n",
    "    print(\"After\")\n",
    "    # Next 3 lines are only when apply function is used\n",
    "    #extra = new_geom.to_frame()\n",
    "    #print(type(extra))\n",
    "    #extra.head(3)\n",
    "\n",
    "    #new_geom.head(3)\n",
    "    #print(type(new_geom))\n",
    "    busesparaderos = new_geom.to_spark()\n",
    "    #busesparaderos.printSchema()\n",
    "    temp_minimos = busesparaderos.groupBy('fechahora').agg(F.min(busesparaderos.distancia).alias(\"md\"))\n",
    "    temp_minimos.printSchema()\n",
    "    minimos = temp_minimos.select(temp_minimos.md,temp_minimos.fechahora.alias(\"fh\"))\n",
    "\n",
    "    resultados = busesparaderos.join(minimos, [busesparaderos.distancia == minimos.md , busesparaderos.fechahora == minimos.fh], 'inner')\n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ff1bc2-944e-4c6a-8b6d-d1d6f24b8f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Row(name='Alice') <Row(dict_items([('name', 'Alice')]))>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "word = '0101000020E61000000200008017AD51C0010000C01CBC40C0'\n",
    "punto = wkb.loads(word, hex=True)\n",
    "x,y = punto.xy\n",
    "word = Row(name=\"Alice\")\n",
    "arr = Point()\n",
    "print(len(word), word, Row(word.asDict().items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46bac479-ddc4-4ea4-8f0f-1595f2f5d345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "root\n",
      " |-- fechahora: string (nullable = true)\n",
      " |-- md: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = createData(buses,paraderos2, paraderosrutas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c90f8-f123-4438-a35e-b567b126cdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- simt: string (nullable = true)\n",
      " |-- bus: string (nullable = true)\n",
      " |-- fechahora: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- lng: string (nullable = true)\n",
      " |-- velocidad: string (nullable = true)\n",
      " |-- pasajeros: string (nullable = true)\n",
      " |-- vendor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- geom_bus: string (nullable = true)\n",
      " |-- servicio: string (nullable = true)\n",
      " |-- nombre_par: string (nullable = true)\n",
      " |-- parder_geom: string (nullable = true)\n",
      " |-- parader: string (nullable = true)\n",
      " |-- stop_name: string (nullable = true)\n",
      " |-- parder_lat: string (nullable = true)\n",
      " |-- parder_lon: string (nullable = true)\n",
      " |-- distancia: double (nullable = true)\n",
      " |-- md: double (nullable = true)\n",
      " |-- fh: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                 (0 + 2) / 3][Stage 21:>                 (0 + 0) / 3]\r"
     ]
    }
   ],
   "source": [
    "resultados.printSchema()\n",
    "temp2 = resultados.to_pandas_on_spark().spark.coalesce(3).groupby([\"parader\"]).apply(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4283243-e3c4-4585-bfb5-fff16c47c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtrados = resultados.filter(resultados.vendor == 'E04I')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c36163a-c629-49da-be05-6834eb8f4c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'> <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(filtrados), type(resultados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68bac5bb-3955-4984-be34-795d11f7806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    }
   ],
   "source": [
    "#filtrados.head(3)\n",
    "#filtrados.show(2)\n",
    "muestras = filtrados.to_pandas_on_spark()\n",
    "print(\"End\")\n",
    "new_muestras = muestras.spark.coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fde2d8e1-ebdf-45e1-92cf-a96c1aed73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#muestras.collect()\n",
    "# new_muestras.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d78672-2b31-4b27-9efd-b6fa0092fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f974bc5-fa89-4dc2-8f44-3be2dcba7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_Bus_Cant_Prom_Por_Paredero(source):#Frecuenica\n",
    "    Hour_list_from = ['2018-06-13 00:00:00+00','2018-06-13 01:00:00+00','2018-06-13 02:00:00+00','2018-06-13 03:00:00+00','2018-06-13 04:00:00+00','2018-06-13 05:00:00+00','2018-06-13 06:00:00+00','2018-06-13 07:00:00+00','2018-06-13 08:00:00+00','2018-06-13 09:00:00+00','2018-06-13 10:00:00+00','2018-06-13 11:00:00+00','2018-06-13 12:00:00+00','2018-06-13 13:00:00+00','2018-06-13 14:00:00+00','2018-06-13 15:00:00+00','2018-06-13 16:00:00+00','2018-06-13 18:00:00+00','2018-06-13 19:00:00+00','2018-06-13 20:00:00+00','2018-06-13 21:00:00+00','2018-06-13 22:00:00+00','2018-06-13 23:00:00+00']\n",
    "    Hour_list_to = ['2018-06-13 00:59:59+00','2018-06-13 01:59:59+00','2018-06-13 02:59:59+00','2018-06-13 03:59:59+00','2018-06-13 04:59:59+00','2018-06-13 05:59:59+00','2018-06-13 06:59:59+00','2018-06-13 07:59:59+00','2018-06-13 08:59:59+00','2018-06-13 09:59:59+00','2018-06-13 10:59:59+00','2018-06-13 11:59:59+00','2018-06-13 12:59:59+00','2018-06-13 13:59:59+00','2018-06-13 14:59:59+00','2018-06-13 15:59:59+00','2018-06-13 16:59:59+00','2018-06-13 18:59:59+00','2018-06-13 19:59:59+00','2018-06-13 20:59:59+00','2018-06-13 21:59:59+00','2018-06-13 22:59:59+00','2018-06-13 23:59:59+00']\n",
    "    dataframe_ruta = source#.toPandas()\n",
    "    \n",
    "    #temp2 = dataframe_ruta.groupby(['parader'], group_keys=True, as_index=True).apply(lambda x: x)\n",
    "    temp2 = dataframe_ruta.groupBy(['parader']).apply(lambda x: x)\n",
    "    return \n",
    "    #temp4 = dataframe_ruta.groupby(['parder_geom'], group_keys=True, as_index=True).apply(lambda x: x)\n",
    "    list_paredors = temp2.index.levels[0]\n",
    "    #temp3 = temp2.loc['PA146']\n",
    "    #a = temp2.loc['PA146']['fechahora'].tolist()[0]\n",
    "\n",
    "    dataframe_buses = {'paradero':[],'promedio_bus_hora':[],'geo_parader':[],'geo_parader_lat':[],'geo_parader_lon':[],'calificacion':[]}\n",
    "    #print(len(Hour_list_from))\n",
    "    #print(len(Hour_list_to))\n",
    "\n",
    "    for i in list_paredors: \n",
    "        x = temp2.loc[i]\n",
    "        List_total_buses = list()\n",
    "        geo = x['parder_geom'].iat[0]\n",
    "        geo_lat = x['parder_lat'].iat[0]\n",
    "        geo_lon = x['parder_lon'].iat[0]\n",
    "        #paradero_list = list()\n",
    "        #print(geo)\n",
    "        for j in range(23):\n",
    "            #print(j)\n",
    "            df = x[(x['fechahora'] >= Hour_list_from[j]) & (x['fechahora'] <= Hour_list_to[j])]\n",
    "            List_total_buses.append(df['bus'].nunique())\n",
    "            #Lista_interval.append(Hour_list_from[j]+' to '+Hour_list_to[j])\n",
    "        mean = (sum(List_total_buses) / 24)\n",
    "        dataframe_buses['paradero'].append(i)\n",
    "        dataframe_buses['promedio_bus_hora'].append(mean)\n",
    "        dataframe_buses['geo_parader'].append(geo)\n",
    "        dataframe_buses['geo_parader_lat'].append(geo_lat)\n",
    "        dataframe_buses['geo_parader_lon'].append(geo_lon)\n",
    "        if mean < 5:\n",
    "            dataframe_buses['calificacion'].append(0)\n",
    "        elif mean > 6:\n",
    "            dataframe_buses['calificacion'].append(1)\n",
    "        else:\n",
    "            dataframe_buses['calificacion'].append(2)\n",
    "\n",
    "\n",
    "        \n",
    "        #dataframe_buses['rango_hora'] += Lista_interval\n",
    "\n",
    "\n",
    "    dataframe_total_busparadero = pd.DataFrame(dataframe_buses)\n",
    "\n",
    "#print(resultados.to_pandas_on_spark().shape)\n",
    "#get_Bus_Cant_Prom_Por_Paredero(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a423b5-952e-4457-98de-e4b7aee4e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get item from a Series\n",
      "Requirement already satisfied: koalas in /usr/local/lib/python3.9/dist-packages (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.9/dist-packages (from koalas) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=0.10 in /usr/local/lib/python3.9/dist-packages (from koalas) (10.0.1)\n",
      "Requirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.9/dist-packages (from koalas) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.2->koalas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.2->koalas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23.2->koalas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# delete.item()\n",
    "print(\"Get item from a Series\")\n",
    "!pip install koalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfa0ceb6-6c59-4822-91cd-cdd1a24a7e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.2.0\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>suma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  suma\n",
       "0  1  4     5\n",
       "1  2  5     7\n",
       "2  3  6     9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import databricks.koalas as ks\n",
    "\n",
    "ks.set_option('compute.default_index_type', 'distributed')\n",
    "#data.count()\n",
    "#kf = ks.DataFrame(data)\n",
    "#len(kf)\n",
    "#kf.apply(length, 1)\n",
    "#df2 = spark.sql(\"select * from mytable\")\n",
    "#df2.count()\n",
    "\n",
    "\n",
    "kdf = ks.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "def pandas_plus(pser):\n",
    "    suma = sum(pser)\n",
    "    temp = pser\n",
    "    temp[\"suma\"] = suma\n",
    "    return temp  # allows an arbitrary length\n",
    "#kdf.print_schema()\n",
    "kdf.apply(pandas_plus, axis='columns')\n",
    "\n",
    "#example = ps.DataFrame(resultados)\n",
    "#other = example.groupby(['parader']).apply(lambda x: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce2651c-d2b0-4d43-9105-97b620e7a1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
