{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ea0484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ba55ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/25 03:52:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b26c919a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[bus: string, fechahora: string, lat: string, lng: string, velocidad: string, pasajeros: string, geom: string, vendor: string, id: string]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataframe=spark.read.option(\"header\",True).csv(\"data.csv\")\n",
    "\n",
    "dataframe.write.option(\"header\",True).partitionBy([\"vendor\", \"bus\"]).mode(\"overwrite\").csv(\"resultados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Bus_Cant_Prom_Por_Paredero():#Frecuenica\n",
    "    Hour_list_from = ['2018-06-13 00:00:00+00','2018-06-13 01:00:00+00','2018-06-13 02:00:00+00','2018-06-13 03:00:00+00','2018-06-13 04:00:00+00','2018-06-13 05:00:00+00','2018-06-13 06:00:00+00','2018-06-13 07:00:00+00','2018-06-13 08:00:00+00','2018-06-13 09:00:00+00','2018-06-13 10:00:00+00','2018-06-13 11:00:00+00','2018-06-13 12:00:00+00','2018-06-13 13:00:00+00','2018-06-13 14:00:00+00','2018-06-13 15:00:00+00','2018-06-13 16:00:00+00','2018-06-13 18:00:00+00','2018-06-13 19:00:00+00','2018-06-13 20:00:00+00','2018-06-13 21:00:00+00','2018-06-13 22:00:00+00','2018-06-13 23:00:00+00']\n",
    "\n",
    "    Hour_list_to = ['2018-06-13 00:59:59+00','2018-06-13 01:59:59+00','2018-06-13 02:59:59+00','2018-06-13 03:59:59+00','2018-06-13 04:59:59+00','2018-06-13 05:59:59+00','2018-06-13 06:59:59+00','2018-06-13 07:59:59+00','2018-06-13 08:59:59+00','2018-06-13 09:59:59+00','2018-06-13 10:59:59+00','2018-06-13 11:59:59+00','2018-06-13 12:59:59+00','2018-06-13 13:59:59+00','2018-06-13 14:59:59+00','2018-06-13 15:59:59+00','2018-06-13 16:59:59+00','2018-06-13 18:59:59+00','2018-06-13 19:59:59+00','2018-06-13 20:59:59+00','2018-06-13 21:59:59+00','2018-06-13 22:59:59+00','2018-06-13 23:59:59+00']\n",
    "    #*************************\n",
    "    #print(type(result.index))\n",
    "    #print(result.index.names)\n",
    "    #print(result.index.values)\n",
    "    #print(result.index.levels)\n",
    "\n",
    "    dataframe_ruta = pd.read_csv(\"./E04I.csv\")\n",
    "    temp2 = dataframe_ruta.groupby(['parader'], group_keys=True, as_index=True).apply(lambda x: x)\n",
    "    #temp4 = dataframe_ruta.groupby(['parder_geom'], group_keys=True, as_index=True).apply(lambda x: x)\n",
    "    list_paredors = temp2.index.levels[0]\n",
    "    #temp3 = temp2.loc['PA146']\n",
    "    #a = temp2.loc['PA146']['fechahora'].tolist()[0]\n",
    "\n",
    "    dataframe_buses = {'paradero':[],'promedio_bus_hora':[],'geo_parader':[],'geo_parader_lat':[],'geo_parader_lon':[],'calificacion':[]}\n",
    "    #print(len(Hour_list_from))\n",
    "    #print(len(Hour_list_to))\n",
    "\n",
    "    for i in list_paredors: \n",
    "        x = temp2.loc[i]\n",
    "        List_total_buses = list()\n",
    "        geo = x['parder_geom'].iat[0]\n",
    "        geo_lat = x['parder_lat'].iat[0]\n",
    "        geo_lon = x['parder_lon'].iat[0]\n",
    "        paradero_list = list()\n",
    "        #print(geo)\n",
    "        for j in range(23):\n",
    "            #print(j)\n",
    "            df = x[(x['fechahora'] >= Hour_list_from[j]) & (x['fechahora'] <= Hour_list_to[j])]\n",
    "            List_total_buses.append(df['bus'].nunique())\n",
    "            #Lista_interval.append(Hour_list_from[j]+' to '+Hour_list_to[j])\n",
    "        mean = (sum(List_total_buses) / len(List_total_buses))\n",
    "        dataframe_buses['paradero'].append(i)\n",
    "        dataframe_buses['promedio_bus_hora'].append(mean)\n",
    "        dataframe_buses['geo_parader'].append(geo)\n",
    "        dataframe_buses['geo_parader_lat'].append(geo_lat)\n",
    "        dataframe_buses['geo_parader_lon'].append(geo_lon)\n",
    "        if mean < 5:\n",
    "            dataframe_buses['calificacion'].append(0)\n",
    "        elif mean > 6:\n",
    "            dataframe_buses['calificacion'].append(1)\n",
    "        else:\n",
    "            dataframe_buses['calificacion'].append(2)\n",
    "\n",
    "\n",
    "        \n",
    "        #dataframe_buses['rango_hora'] += Lista_interval\n",
    "\n",
    "\n",
    "    dataframe_total_busparadero = pd.DataFrame(dataframe_buses)\n",
    "    print(dataframe_total_busparadero)\n",
    "    dataframe_total_busparadero.to_csv('E04I_NOTED.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
